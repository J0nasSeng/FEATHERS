{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from opacus import PrivacyEngine\n",
    "import torch.nn.functional as F\n",
    "from operations import *\n",
    "from genotypes import PRIMITIVES\n",
    "from genotypes import Genotype\n",
    "from opacus.grad_sample import GradSampleModule, register_grad_sampler\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "from typing import Dict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding sample-wise Gradients for Supernet\n",
    "\n",
    "In this notebook we derive how we come up with sample-wise gradients for our supernet we use to perform differentiable NAS. For this we note that the supernet is based on a convex, weighted combination of operations which are all applied to the same input, that is each _mixed operation_ is defined as follows:\n",
    "\\begin{equation}\n",
    "    m = \\sum_{o \\in O} \\alpha_o \\cdot o(x)\n",
    "\\end{equation}\n",
    "Each $o$ is a convolution/pooling operation or a regular neural network, thus opacus already knows how to compute sample-based gradients for all parameters of each $o$. Thus, with a smart design of our supernet-architecture we can avoid the computation of sample-wise gradients for all the operations we have in use and pass the heavy lifting to opacus. The problem then reduces to providing opacus with sample-wise gradients w.r.t $\\alpha_o$ for each $o$.\n",
    "\n",
    "For this, let's see how we compute the gradients of an arbitrary loss w.r.t. the alpha-parameters in a simple setup: We only have 3 operations, each associated with a certaing weight $\\alpha_o$. The mixed operation is then followed by a linear transformation producing the output, thus the network reads:\n",
    "\\begin{equation}\n",
    "    \\hat{y} = \\bigg(\\sum_{o \\in O} \\alpha_o \\cdot o(x) \\bigg) \\cdot \\mathbf{W}\n",
    "\\end{equation}\n",
    "\n",
    "The following shwos the forward pass of such a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [torch.randn(1, 5) for _ in range(0, 4)]\n",
    "alphas = nn.Parameter(torch.ones(4) / 4, requires_grad=True)\n",
    "W = nn.Parameter(torch.randn(5, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxed_alphas = torch.softmax(alphas, dim=0)\n",
    "softmaxed_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1746]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mop = sum(alphas[i] * X[i] for i in range(0, 4))\n",
    "y = torch.matmul(mop, W)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed with computing a loss and updating the parameters. To keep things easy, let's pretend our loss is just the sum of all elements in our output. We then can compute the gradients w.r.t. each $\\alpha_o$ easily by calling the backward-method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9922, -0.8952,  0.3707, -2.1817])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.sum(y)\n",
    "l.backward()\n",
    "alphas.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient can be expressed as follows:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\ell}{\\partial \\alpha_o} = \\sum_{i=1}^n \\frac{\\partial \\ell}{\\partial z^{(0)}_k} \\cdot \\frac{\\partial z^{(0)}_k}{\\partial \\alpha_o}\n",
    "\\end{equation}\n",
    "Here we sum over all gradients of our $n$ outputs w.r.t. $\\alpha_o$. $z^{(0)}_k$ denotes the $k$-th element of the last layer (output). Expanding this further yields:\n",
    "\\begin{align}\n",
    "    \\frac{\\partial \\ell}{\\partial \\alpha_o} = & \\sum_{i=1}^n \\frac{\\partial \\ell}{\\partial z^{(0)}_k} \\cdot \\sum_{d=1}^{|L_1|} \\mathbf{W}^{(1)}_{d k} \\cdot \\frac{\\partial z_d^{(1)}}{\\partial \\alpha_o} \\\\\n",
    "    & \\sum_{i=1}^n \\frac{\\partial \\ell}{\\partial z^{(0)}_k} \\cdot \\sum_{d=1}^{|L_1|} \\mathbf{W}^{(1)}_{d k} \\cdot \\frac{\\partial \\sum_{o \\in O} \\big( \\alpha_o o(z^{(2)}) \\big)}{\\partial \\alpha_o} \\\\\n",
    "    &  \\sum_{i=1}^n \\frac{\\partial \\ell}{\\partial z^{(0)}_k} \\cdot \\sum_{d=1}^{|L_1|} \\mathbf{W}^{(1)}_{d k} \\cdot o(z^{(2)})_d\n",
    "\\end{align}\n",
    "Since $\\frac{\\partial \\ell}{\\partial z^{(0)}_k} = 1$ we obtain:\n",
    "\\begin{align}\n",
    "    \\frac{\\partial \\ell}{\\partial \\alpha_o} & = \\sum_{k=1}^n \\sum_{d=1}^{|L_1|} \\mathbf{W}_{d k}^{(1)} \\cdot o(z^{(2)})_d \\\\\n",
    "\\end{align}\n",
    "This reduces to:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\ell}{\\partial \\alpha_o} = \\sum_{k=1}^n \\big(\\mathbf{W}^{(1)^T}\\big)_k \\cdot o(z^{(2)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8533]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 * X[0].matmul(W) # 1 = derivative w.r.t. the output of mixed operation, rest as derived above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the Minimal Example\n",
    "Now we will start extending the above approach. As you can see computing the gradients and deriving the forumlas can get cumbersome really quickly. That's why we should use opacus' capabilities of computing sample-wise gradients for as many modules as possible. Below we define 3 operations that will be used withing our NAS-approach. For these modules opacus already knows how to compute sample-wise gradients, thus we can just go ahead and use them as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Op1(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Op2(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Op3(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "PRIMS = [Op1, Op2, Op3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tricky part is the mixed operation-module. If we would use plain pytorch, we just could go ahead and build one `MixedOp` module taking care of calling each operation and compute the convex combination of the operation's output. However, since opacus does not know the MixedOp-module it cannot compute the sample-wise gradients w.r.t. the alpha-parameters. Thus we have to tell opacus how to compute the gradients. For this opacus provides the activations (input to our module) and the gradients w.r.t. the outputs of our module. If we would go for a \"plain-pytorch approach\" this would require us to compute the gradients w.r.t. alpha-parameters and the model-parameters of each operation \"by hand\". Since this is cumbersome and not error-prone, we split up the MixedOp into two parts: One `ParallelOp` which does nothing but applying each operation on the same input data and a `MixedOp` (please don't get confused by the naming) which just cares about computing the convex combination of the oerpation's output computed by the ParallelOp. \n",
    "\n",
    "This way we can compute the gradients w.r.t. the alphas easily by just applying the same reasoning as above. The gradients can then be computed by computing the vector-product of the activations and the gradients w.r.t. the MixedOp (which are both provided by opacus). Mathematically this can be expressed as follows assuming we have an $n \\times i$-dimensional real activation matrix $\\mathbf{a}$ and an $i$-dimensional real vector $\\nabla \\mathbf{m}$ representing the gradients w.r.t. our mixed operation. We have an activation of $n \\times i$ because we compute the convex combination of $n$ operations, each producing outputs of dimension $i$. Since each of the $n$ operations is associated with a weight $\\alpha_j$, we aim to compute the gradient w.r.t. each of the $n$ weights, thus we aim to obtain a $n$-dimensional gradient vector for one sample and a $B \\times n$-dimensional gradient matrix for a batch of size $B$. We can easily compute this using an einsum:\n",
    "\\begin{equation}\n",
    "    \\nabla_{j} \\alpha_m = \\sum_{k=1}^i \\nabla \\mathbf{m}_i \\cdot \\mathbf{a}_{m k}\n",
    "\\end{equation}\n",
    "Here $\\nabla_j \\alpha_m$ is the $j$-th element of the gradient vector w.r.t. to the alphas associated with operation $m$. As we can see this is just a more general version of the equation we've derived above.\n",
    "\n",
    "But this is not all we have to do: Remember we have the ParallelOp computing all the operation's outputs. This module does not have any parameters, thus there is nothing we can update and so there are also no gradients for this module. Thus we can tell opacus that there is nothing to compute in the backward pass. Details on the implementation can be obtained below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelOp(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, h_dim, out_dim) -> None:\n",
    "        super(ParallelOp, self).__init__()\n",
    "        self._ops = nn.ModuleList()\n",
    "        for primitive in PRIMS:\n",
    "            self._ops.append(primitive(in_dim, h_dim, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        operation_outs = []\n",
    "        for op in self._ops:\n",
    "            out = op(x)\n",
    "            operation_outs.append(out)\n",
    "        return torch.stack(operation_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedOp(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MixedOp, self).__init__()\n",
    "        self.alphas = nn.Parameter(torch.zeros(len(PRIMS)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = torch.softmax(self.alphas, 0)\n",
    "        return sum(w * op_out for w, op_out in zip(weights, x))\n",
    "\n",
    "    def arch_params(self):\n",
    "        return [self.alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, hdim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hdim)\n",
    "        self.fc2 = nn.Linear(hdim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.hstack(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'in_dim', 'hdim', and 'out_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jonas/dev/automl/HANF/hanf_dp/dp_supernet_derivation.ipynb Cell 16\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/dp_supernet_derivation.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mixed_op \u001b[39m=\u001b[39m MixedOp()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/dp_supernet_derivation.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m net \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(ParallelOp(\u001b[39m28\u001b[39m\u001b[39m*\u001b[39m\u001b[39m28\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m256\u001b[39m), mixed_op, LastLayer())\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/dp_supernet_derivation.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optim \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(mixed_op\u001b[39m.\u001b[39march_params(), \u001b[39m0.01\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/dp_supernet_derivation.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'in_dim', 'hdim', and 'out_dim'"
     ]
    }
   ],
   "source": [
    "mixed_op = MixedOp()\n",
    "net = nn.Sequential(ParallelOp(28*28, 512, 256), mixed_op, LastLayer())\n",
    "optim = torch.optim.SGD(mixed_op.arch_params(), 0.01)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0,), (1,))])\n",
    "train_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=True, transform=transform)\n",
    "val_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=False, transform=transform)\n",
    "train_loader = DataLoader(train_data, 64)\n",
    "val_loader = DataLoader(val_data, 64)\n",
    "\n",
    "@register_grad_sampler(ParallelOp)\n",
    "def grad_sampler_parallel_op(layer: MixedOp, activations: torch.Tensor, backprops: torch.Tensor):\n",
    "    return {}\n",
    "\n",
    "@register_grad_sampler(MixedOp)\n",
    "def grad_sampler_mixed_op(layer: MixedOp, activations: torch.Tensor, backprops: torch.Tensor):\n",
    "    grad = torch.einsum('nbi,bi->nb', activations, backprops)\n",
    "    ret = {\n",
    "        layer.alphas: grad\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "pe = PrivacyEngine()\n",
    "netc = deepcopy(net)\n",
    "net_, optim_, train_loader_ = pe.make_private(module=net, optimizer=optim, data_loader=train_loader, noise_multiplier=1., max_grad_norm=1.)\n",
    "x_first, y_first = next(iter(train_loader_))\n",
    "x_first = x_first.reshape((x_first.shape[0], 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_first = torch.randn((64, 784))\n",
    "y_first = torch.ones(64, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'netc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jonas/dev/automl/HANF/hanf_dp/dp_supernet_derivation.ipynb Cell 18\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/dp_supernet_derivation.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m pnetc, pnet_ \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(netc\u001b[39m.\u001b[39mparameters(), net_\u001b[39m.\u001b[39mparameters()):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/dp_supernet_derivation.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mall(pnet_\u001b[39m.\u001b[39mdata \u001b[39m==\u001b[39m pnetc\u001b[39m.\u001b[39mdata))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'netc' is not defined"
     ]
    }
   ],
   "source": [
    "for pnetc, pnet_ in zip(netc.parameters(), net_.parameters()):\n",
    "    print(torch.all(pnet_.data == pnetc.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "y_pred = net_(x_first)\n",
    "l = loss(y_pred, y_first)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = netc(x_first)\n",
    "l = loss(y_pred, y_first)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for pnetc, pnet_ in zip(netc.parameters(), net_.parameters()):\n",
    "    print(torch.all(pnetc.grad.data == pnet_.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Does this make sense? \n",
    "> \n",
    "> Yes it does since the optimizer performs the clipping and adds noise, thus the gradients are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building up a Cell\n",
    "Now that we know how to build MixedOps with minimal effort, we can use the MixedOps in order to build up cells and we will use the cells in turn to build up our architecture search space. Since cells don't introduce any additional parameters, building a cell based on a set of MixedOps shoud be straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell(nn.Module):\n",
    "    # TODO: Do the same as above and check that gradients are correct!\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.nodes = nn.ModuleList()\n",
    "        # initialize stem modules with some conv-operation self.stem0, self.stem1 = \n",
    "        curr_in_dim = 28*28\n",
    "        curr_hdim = int(0.75 * curr_in_dim)\n",
    "        curr_out_dim = int(0.75 * curr_hdim)\n",
    "        dims = [curr_out_dim]\n",
    "        for i in range(5):\n",
    "            if i == 0:\n",
    "                mop = nn.Sequential(ParallelOp(curr_in_dim, curr_hdim, curr_out_dim), MixedOp())\n",
    "            else:\n",
    "                in_dim = sum(dims)\n",
    "                hdim = int(0.75*in_dim)\n",
    "                out_dim = int(0.75*hdim)\n",
    "                dims.append(out_dim)\n",
    "                mop = nn.Sequential(ParallelOp(in_dim, hdim, out_dim), MixedOp())\n",
    "            self.nodes.append(mop)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inp = [self.nodes[0](x)]\n",
    "        for op in self.nodes[1:]:\n",
    "            print(torch.hstack(inp).shape)\n",
    "            out = op(torch.hstack(inp))\n",
    "            inp.append(out)\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Supernet\n",
    "Now that we know how to build up a cell we can proceed and glue several cells to one network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cell1 = Cell()\n",
    "        self.linear1 = nn.Linear(2623, 28*28)\n",
    "        self.cell2 = Cell()\n",
    "        self.out = LastLayer(2623, 256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cell1(x)\n",
    "        x = torch.hstack(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.cell2(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(net: nn.Module, param_type='arch'):\n",
    "    parameters = []\n",
    "    for name, param in net.named_parameters():\n",
    "        if param_type == 'arch':\n",
    "            if 'alphas' in name:\n",
    "                parameters.append(param)\n",
    "        elif param_type == 'model':\n",
    "            if 'alphas' not in name:\n",
    "                parameters.append(param)\n",
    "        else:\n",
    "            raise ValueError('Unsupported parameter type, must be either arch or model')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/opacus/privacy_engine.py:130: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#net = nn.Sequential(Cell(), LastLayer(2623, 256, 10))\n",
    "net = Network()\n",
    "params = get_params(net, 'arch')\n",
    "optim = torch.optim.SGD(params, 0.01)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0,), (1,))])\n",
    "train_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=True, transform=transform)\n",
    "val_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=False, transform=transform)\n",
    "train_loader = DataLoader(train_data, 64)\n",
    "val_loader = DataLoader(val_data, 64)\n",
    "\n",
    "@register_grad_sampler(ParallelOp)\n",
    "def grad_sampler_parallel_op(layer: MixedOp, activations: torch.Tensor, backprops: torch.Tensor):\n",
    "    return {}\n",
    "\n",
    "@register_grad_sampler(MixedOp)\n",
    "def grad_sampler_mixed_op(layer: MixedOp, activations: torch.Tensor, backprops: torch.Tensor):\n",
    "    grad = torch.einsum('nbi,bi->nb', activations, backprops)\n",
    "    ret = {\n",
    "        layer.alphas: grad\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "pe = PrivacyEngine()\n",
    "netc = deepcopy(net)\n",
    "net_, optim_, train_loader_ = pe.make_private(module=net, optimizer=optim, data_loader=train_loader, noise_multiplier=1., max_grad_norm=1.)\n",
    "x_first, y_first = next(iter(train_loader_))\n",
    "x_first = x_first.reshape((x_first.shape[0], 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_first = torch.randn((64, 784))\n",
    "y_first = torch.ones(64, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 441])\n",
      "torch.Size([64, 688])\n",
      "torch.Size([64, 1075])\n",
      "torch.Size([64, 1679])\n",
      "torch.Size([64, 441])\n",
      "torch.Size([64, 688])\n",
      "torch.Size([64, 1075])\n",
      "torch.Size([64, 1679])\n",
      "torch.Size([64, 441])\n",
      "torch.Size([64, 688])\n",
      "torch.Size([64, 1075])\n",
      "torch.Size([64, 1679])\n",
      "torch.Size([64, 441])\n",
      "torch.Size([64, 688])\n",
      "torch.Size([64, 1075])\n",
      "torch.Size([64, 1679])\n"
     ]
    }
   ],
   "source": [
    "y_pred = net_(x_first)\n",
    "l = loss(y_pred, y_first)\n",
    "l.backward()\n",
    "\n",
    "y_pred = netc(x_first)\n",
    "l = loss(y_pred, y_first)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for pnetc, pnet_ in zip(netc.parameters(), net_.parameters()):\n",
    "    print(torch.all(pnetc.grad.data == pnet_.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer to CNNs\n",
    "In the above example we have used standard MLPs as our operations. However, our ultimate goal is to perform image classification, thus our operation-space consists of convolution- and pooling-operations. This requires us to adapt the computation of the gradients slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedOp(nn.Module):\n",
    "\n",
    "  def __init__(self, C, stride):\n",
    "    super(MixedOp, self).__init__()\n",
    "    self._ops = nn.ModuleList()\n",
    "    for primitive in PRIMITIVES:\n",
    "      op = OPS[primitive](C, stride, False)\n",
    "      if 'pool' in primitive:\n",
    "        op = nn.Sequential(op, nn.GroupNorm(num_groups=1, num_channels=C, affine=False))\n",
    "      self._ops.append(op)\n",
    "\n",
    "  def forward(self, x, weights):\n",
    "    return sum(w * op(x) for w, op in zip(weights, self._ops))\n",
    "\n",
    "\n",
    "class Cell(nn.Module):\n",
    "\n",
    "  def __init__(self, steps, multiplier, C_prev_prev, C_prev, C, reduction, reduction_prev):\n",
    "    super(Cell, self).__init__()\n",
    "    self.reduction = reduction\n",
    "\n",
    "    if reduction_prev:\n",
    "      self.preprocess0 = FactorizedReduce(C_prev_prev, C, affine=False)\n",
    "    else:\n",
    "      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0, affine=False)\n",
    "    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0, affine=False)\n",
    "    self._steps = steps\n",
    "    self._multiplier = multiplier\n",
    "\n",
    "    self._ops = nn.ModuleList()\n",
    "    self._bns = nn.ModuleList()\n",
    "    for i in range(self._steps):\n",
    "      for j in range(2+i):\n",
    "        stride = 2 if reduction and j < 2 else 1\n",
    "        op = MixedOp(C, stride)\n",
    "        self._ops.append(op)\n",
    "\n",
    "  def forward(self, s0, s1, weights):\n",
    "    s0 = self.preprocess0(s0)\n",
    "    s1 = self.preprocess1(s1)\n",
    "\n",
    "    states = [s0, s1]\n",
    "    offset = 0\n",
    "    for i in range(self._steps):\n",
    "      s = sum(self._ops[offset+j](h, weights[offset+j]) for j, h in enumerate(states))\n",
    "      offset += len(states)\n",
    "      states.append(s)\n",
    "\n",
    "    return torch.cat(states[-self._multiplier:], dim=1)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "  def __init__(self, C, num_classes, layers, criterion, device, in_channels=3, steps=4, multiplier=4, stem_multiplier=3):\n",
    "    super(Network, self).__init__()\n",
    "    self._C = C\n",
    "    self._num_classes = num_classes\n",
    "    self._layers = layers\n",
    "    self._criterion = criterion\n",
    "    self._steps = steps\n",
    "    self._multiplier = multiplier\n",
    "    self.device = device\n",
    "\n",
    "    C_curr = stem_multiplier*C\n",
    "    self.stem = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, C_curr, 3, padding=1, bias=False),\n",
    "      nn.GroupNorm(num_groups=1, num_channels=C_curr),\n",
    "    )\n",
    " \n",
    "    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C\n",
    "    self.cells = nn.ModuleList()\n",
    "    reduction_prev = False\n",
    "    for i in range(layers):\n",
    "      if i in [layers//3, 2*layers//3]:\n",
    "        C_curr *= 2\n",
    "        reduction = True\n",
    "      else:\n",
    "        reduction = False\n",
    "      cell = Cell(steps, multiplier, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\n",
    "      reduction_prev = reduction\n",
    "      self.cells += [cell]\n",
    "      C_prev_prev, C_prev = C_prev, multiplier*C_curr\n",
    "\n",
    "    self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "    self.classifier = nn.Linear(C_prev, num_classes)\n",
    "\n",
    "    self._initialize_alphas()\n",
    "\n",
    "  def new(self):\n",
    "    model_new = Network(self._C, self._num_classes, self._layers, self._criterion, self.device).to(self.device)\n",
    "    for x, y in zip(model_new.arch_parameters(), self.arch_parameters()):\n",
    "        x.data.copy_(y.data)\n",
    "    return model_new\n",
    "\n",
    "  def forward(self, input):\n",
    "    s0 = s1 = self.stem(input)\n",
    "    for i, cell in enumerate(self.cells):\n",
    "      if cell.reduction:\n",
    "        weights = F.softmax(self.alphas_reduce, dim=-1)\n",
    "      else:\n",
    "        weights = F.softmax(self.alphas_normal, dim=-1)\n",
    "      s0, s1 = s1, cell(s0, s1, weights)\n",
    "    out = self.global_pooling(s1)\n",
    "    logits = self.classifier(out.view(out.size(0),-1))\n",
    "    return logits\n",
    "\n",
    "  def _initialize_alphas(self):\n",
    "    k = sum(1 for i in range(self._steps) for n in range(2+i))\n",
    "    num_ops = len(PRIMITIVES)\n",
    "\n",
    "    self.alphas_normal = nn.Parameter(1e-3*torch.randn(k, num_ops).to(self.device), requires_grad=True)\n",
    "    self.alphas_reduce = nn.Parameter(1e-3*torch.randn(k, num_ops).to(self.device), requires_grad=True)\n",
    "    self._arch_parameters = [\n",
    "      self.alphas_normal,\n",
    "      self.alphas_reduce,\n",
    "    ]\n",
    "\n",
    "  def arch_parameters(self):\n",
    "    return self._arch_parameters\n",
    "\n",
    "  def genotype(self):\n",
    "\n",
    "    def _parse(weights):\n",
    "      gene = []\n",
    "      n = 2\n",
    "      start = 0\n",
    "      for i in range(self._steps):\n",
    "        end = start + n\n",
    "        W = weights[start:end].copy()\n",
    "        edges = sorted(range(i + 2), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[:2]\n",
    "        for j in edges:\n",
    "          k_best = None\n",
    "          for k in range(len(W[j])):\n",
    "            if k != PRIMITIVES.index('none'):\n",
    "              if k_best is None or W[j][k] > W[j][k_best]:\n",
    "                k_best = k\n",
    "          gene.append((PRIMITIVES[k_best], j))\n",
    "        start = end\n",
    "        n += 1\n",
    "      return gene\n",
    "\n",
    "    gene_normal = _parse(F.softmax(self.alphas_normal, dim=-1).data.cpu().numpy())\n",
    "    gene_reduce = _parse(F.softmax(self.alphas_reduce, dim=-1).data.cpu().numpy())\n",
    "\n",
    "    concat = range(2+self._steps-self._multiplier, self._steps+2)\n",
    "    genotype = Genotype(\n",
    "      normal=gene_normal, normal_concat=concat,\n",
    "      reduce=gene_reduce, reduce_concat=concat\n",
    "    )\n",
    "    return genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cpu')\n",
    "model = Network(16, 10, 7, criterion, device, in_channels=1) # Cell(4, 3, 16, 36, 48, False, False)\n",
    "optim_arch = torch.optim.SGD(get_params(model, 'arch'), 0.01)\n",
    "optim_model = torch.optim.SGD(get_params(model, 'model'), 0.01)\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0,), (1,))])\n",
    "train_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=True, transform=transform)\n",
    "val_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=False, transform=transform)\n",
    "train_loader = DataLoader(train_data, 64)\n",
    "val_loader = DataLoader(val_data, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 28, 28])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "for e in range(0, 10):\n",
    "    running_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        y_hat = model(x)\n",
    "        break\n",
    "        l = loss(y_hat, y)\n",
    "        running_loss += l\n",
    "\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    print(f\"Loss: {running_loss / len(train_loader)} \\t Epoch: {e}\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelOp(nn.Module):\n",
    "\n",
    "  def __init__(self, C, stride) -> None:\n",
    "    super().__init__()\n",
    "    self._ops = nn.ModuleList()\n",
    "    for primitive in PRIMITIVES:\n",
    "      op = OPS[primitive](C, stride, False)\n",
    "      if 'pool' in primitive:\n",
    "        op = nn.Sequential(op, nn.GroupNorm(num_groups=1, num_channels=C, affine=False))\n",
    "      self._ops.append(op)\n",
    "\n",
    "  def forward(self, x):\n",
    "      operation_outs = []\n",
    "      for op in self._ops:\n",
    "          out = op(x)\n",
    "          operation_outs.append(out)\n",
    "      return torch.stack(operation_outs)\n",
    "\n",
    "class MixedOp(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MixedOp, self).__init__()\n",
    "        self.alphas = nn.Parameter(torch.zeros(len(PRIMITIVES)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = torch.softmax(self.alphas, 0)\n",
    "        return sum(w * op_out for w, op_out in zip(weights, x))\n",
    "\n",
    "\n",
    "class Cell(nn.Module):\n",
    "\n",
    "  def __init__(self, steps, multiplier, C_prev_prev, C_prev, C, reduction, reduction_prev):\n",
    "    super(Cell, self).__init__()\n",
    "    self.reduction = reduction\n",
    "\n",
    "    if reduction_prev:\n",
    "      self.preprocess0 = FactorizedReduce(C_prev_prev, C, affine=False)\n",
    "    else:\n",
    "      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0, affine=False)\n",
    "    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0, affine=False)\n",
    "    self._steps = steps\n",
    "    self._multiplier = multiplier\n",
    "\n",
    "    self._ops = nn.ModuleList()\n",
    "    self._bns = nn.ModuleList()\n",
    "    for i in range(self._steps):\n",
    "      for j in range(2+i):\n",
    "        stride = 2 if reduction and j < 2 else 1\n",
    "        op = nn.Sequential(ParallelOp(C, stride), MixedOp())\n",
    "        self._ops.append(op)\n",
    "\n",
    "  def forward(self, s0, s1):\n",
    "    s0 = self.preprocess0(s0)\n",
    "    s1 = self.preprocess1(s1)\n",
    "\n",
    "    states = [s0, s1]\n",
    "    offset = 0\n",
    "    for i in range(self._steps):\n",
    "      s = sum(self._ops[offset+j](h) for j, h in enumerate(states))\n",
    "      offset += len(states)\n",
    "      states.append(s)\n",
    "\n",
    "    return torch.cat(states[-self._multiplier:], dim=1)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "  def __init__(self, C, num_classes, layers, criterion, device, in_channels=3, steps=4, multiplier=4, stem_multiplier=3):\n",
    "    super(Network, self).__init__()\n",
    "    self._C = C\n",
    "    self._num_classes = num_classes\n",
    "    self._layers = layers\n",
    "    self._criterion = criterion\n",
    "    self._steps = steps\n",
    "    self._multiplier = multiplier\n",
    "    self.device = device\n",
    "\n",
    "    C_curr = stem_multiplier*C\n",
    "    self.stem = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, C_curr, 3, padding=1, bias=False),\n",
    "      nn.GroupNorm(num_groups=1, num_channels=C_curr),\n",
    "    )\n",
    " \n",
    "    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C\n",
    "    self.cells = nn.ModuleList()\n",
    "    reduction_prev = False\n",
    "    for i in range(layers):\n",
    "      if i in [layers//3, 2*layers//3]:\n",
    "        C_curr *= 2\n",
    "        reduction = True\n",
    "      else:\n",
    "        reduction = False\n",
    "      cell = Cell(steps, multiplier, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\n",
    "      reduction_prev = reduction\n",
    "      self.cells += [cell]\n",
    "      C_prev_prev, C_prev = C_prev, multiplier*C_curr\n",
    "\n",
    "    self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "    self.classifier = nn.Linear(C_prev, num_classes)\n",
    "\n",
    "  def new(self):\n",
    "    model_new = Network(self._C, self._num_classes, self._layers, self._criterion, self.device).to(self.device)\n",
    "    for x, y in zip(get_params(model_new, 'arch'), get_params(self, 'arch')):\n",
    "        x.data.copy_(y.data)\n",
    "    return model_new\n",
    "\n",
    "  def forward(self, input):\n",
    "    s0 = s1 = self.stem(input)\n",
    "    for i, cell in enumerate(self.cells):\n",
    "      s0, s1 = s1, cell(s0, s1)\n",
    "    out = self.global_pooling(s1)\n",
    "    logits = self.classifier(out.view(out.size(0),-1))\n",
    "    return logits\n",
    "\n",
    "  def genotype(self):\n",
    "\n",
    "    def _parse(weights):\n",
    "      gene = []\n",
    "      n = 2\n",
    "      start = 0\n",
    "      for i in range(self._steps):\n",
    "        end = start + n\n",
    "        W = weights[start:end].copy()\n",
    "        edges = sorted(range(i + 2), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[:2]\n",
    "        for j in edges:\n",
    "          k_best = None\n",
    "          for k in range(len(W[j])):\n",
    "            if k != PRIMITIVES.index('none'):\n",
    "              if k_best is None or W[j][k] > W[j][k_best]:\n",
    "                k_best = k\n",
    "          gene.append((PRIMITIVES[k_best], j))\n",
    "        start = end\n",
    "        n += 1\n",
    "      return gene\n",
    "\n",
    "    gene_normal = _parse(F.softmax(self.alphas_normal, dim=-1).data.cpu().numpy())\n",
    "    gene_reduce = _parse(F.softmax(self.alphas_reduce, dim=-1).data.cpu().numpy())\n",
    "\n",
    "    concat = range(2+self._steps-self._multiplier, self._steps+2)\n",
    "    genotype = Genotype(\n",
    "      normal=gene_normal, normal_concat=concat,\n",
    "      reduce=gene_reduce, reduce_concat=concat\n",
    "    )\n",
    "    return genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_grad_sampler(ParallelOp)\n",
    "def grad_sampler_parallel_op(layer: MixedOp, activations: torch.Tensor, backprops: torch.Tensor):\n",
    "    return {}\n",
    "\n",
    "@register_grad_sampler(MixedOp)\n",
    "def grad_sampler_mixed_op(layer: MixedOp, activations: torch.Tensor, backprops: torch.Tensor):\n",
    "    grad = torch.einsum('nbcwh,bcwh->nb', activations, backprops)\n",
    "    ret = {\n",
    "        layer.alphas: grad\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cpu')\n",
    "model_dp = Network(16, 10, 7, criterion, device, in_channels=1) # Cell(4, 3, 16, 36, 48, False, False)\n",
    "optim_arch = torch.optim.SGD(get_params(model_dp, 'arch'), 0.01)\n",
    "optim_model = torch.optim.SGD(get_params(model_dp, 'model'), 0.01)\n",
    "pe = PrivacyEngine()\n",
    "train_loader_c = deepcopy(train_loader)\n",
    "model_dp_, optim_, train_loader_ = pe.make_private(module=model_dp, optimizer=optim_model, data_loader=train_loader_c, noise_multiplier=1., max_grad_norm=1.)\n",
    "x_first, y_first = next(iter(train_loader_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(x_first)\n",
    "l = criterion(y_pred, y_first)\n",
    "l.backward()\n",
    "\n",
    "y_pred = model_dp_(x_first)\n",
    "l = criterion(y_pred, y_first)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0005, -0.0029,  0.0079,  0.0018, -0.0107, -0.0002,  0.0119, -0.0082])\n",
      "tensor([-0.0022,  0.0033,  0.0007,  0.0025, -0.0028, -0.0162,  0.0047,  0.0102])\n",
      "tensor([ 0.0042,  0.0010, -0.0012, -0.0064,  0.0069, -0.0077, -0.0057,  0.0090])\n",
      "tensor([-0.0015,  0.0068,  0.0027,  0.0057, -0.0012,  0.0086, -0.0203, -0.0008])\n",
      "tensor([-0.0021,  0.0017,  0.0002, -0.0012,  0.0040, -0.0055,  0.0042, -0.0013])\n",
      "tensor([ 3.1051e-03, -4.3753e-03,  1.6559e-03, -1.8433e-03, -4.2395e-03,\n",
      "        -4.5078e-05,  1.5340e-03,  4.2082e-03])\n",
      "tensor([-9.0132e-04, -6.3828e-03, -4.2314e-03, -3.0967e-03, -1.5108e-04,\n",
      "         9.5265e-05,  4.0006e-03,  1.0667e-02])\n",
      "tensor([ 0.0024, -0.0073, -0.0030, -0.0023,  0.0043,  0.0048,  0.0021, -0.0010])\n",
      "tensor([ 0.0053, -0.0035, -0.0034, -0.0052, -0.0044, -0.0092,  0.0126,  0.0080])\n",
      "tensor([-0.0006, -0.0059, -0.0010,  0.0003,  0.0003,  0.0093, -0.0005, -0.0019])\n",
      "tensor([-0.0019, -0.0005,  0.0031,  0.0037,  0.0020,  0.0013, -0.0061, -0.0016])\n",
      "tensor([-1.2281e-03,  6.8668e-04,  6.4192e-04, -9.4325e-05, -8.7534e-03,\n",
      "        -6.1344e-04,  1.1683e-02, -2.3226e-03])\n",
      "tensor([-0.0039, -0.0006,  0.0006,  0.0034, -0.0027, -0.0024,  0.0102, -0.0046])\n",
      "tensor([-0.0006, -0.0028, -0.0007,  0.0022,  0.0015,  0.0012,  0.0099, -0.0107])\n",
      "tensor([ 2.1878e-04,  3.8834e-03, -3.4133e-05,  3.1982e-03, -6.3009e-03,\n",
      "         4.3751e-03,  4.1439e-03, -9.4844e-03])\n",
      "tensor([-0.0039,  0.0080,  0.0104,  0.0098, -0.0159, -0.0033, -0.0084,  0.0033])\n",
      "tensor([-0.0036,  0.0028,  0.0002,  0.0030, -0.0031, -0.0039, -0.0002,  0.0048])\n",
      "tensor([ 0.0016,  0.0003, -0.0003, -0.0017,  0.0073, -0.0037, -0.0063,  0.0028])\n",
      "tensor([ 0.0026,  0.0035,  0.0018,  0.0020,  0.0076, -0.0024, -0.0099, -0.0052])\n",
      "tensor([ 0.0008, -0.0043, -0.0015,  0.0024, -0.0080,  0.0066,  0.0037,  0.0002])\n",
      "tensor([-0.0015, -0.0008,  0.0006,  0.0002, -0.0065,  0.0064,  0.0060, -0.0044])\n",
      "tensor([ 0.0026, -0.0034, -0.0018,  0.0003, -0.0017,  0.0058, -0.0034,  0.0016])\n",
      "tensor([ 0.0028, -0.0032, -0.0027, -0.0011,  0.0030,  0.0086, -0.0097,  0.0023])\n",
      "tensor([-0.0019,  0.0030,  0.0010,  0.0029,  0.0040, -0.0026, -0.0077,  0.0013])\n",
      "tensor([ 0.0013, -0.0020, -0.0009,  0.0010, -0.0002,  0.0063, -0.0017, -0.0038])\n",
      "tensor([-0.0018, -0.0005, -0.0016, -0.0013, -0.0035, -0.0049,  0.0071,  0.0065])\n",
      "tensor([ 0.0009,  0.0012,  0.0004,  0.0004, -0.0025,  0.0011, -0.0013, -0.0002])\n",
      "tensor([ 0.0008,  0.0010,  0.0008,  0.0008, -0.0074,  0.0003, -0.0028,  0.0065])\n",
      "tensor([-0.0001,  0.0010,  0.0005,  0.0050, -0.0193,  0.0044,  0.0005,  0.0081])\n",
      "tensor([ 0.0042, -0.0060, -0.0034,  0.0017, -0.0109, -0.0013,  0.0106,  0.0052])\n",
      "tensor([-0.0008,  0.0032,  0.0040, -0.0023,  0.0049, -0.0067,  0.0016, -0.0040])\n",
      "tensor([-0.0008, -0.0071, -0.0052,  0.0030,  0.0002,  0.0027,  0.0019,  0.0053])\n",
      "tensor([-0.0007,  0.0012, -0.0024, -0.0026,  0.0007,  0.0036,  0.0020, -0.0018])\n",
      "tensor([ 0.0008,  0.0026,  0.0027,  0.0034, -0.0058, -0.0028, -0.0038,  0.0030])\n",
      "tensor([ 0.0004, -0.0047, -0.0064, -0.0008,  0.0079,  0.0026, -0.0004,  0.0014])\n",
      "tensor([ 0.0011, -0.0015, -0.0054, -0.0015,  0.0031, -0.0068, -0.0021,  0.0130])\n",
      "tensor([ 0.0005,  0.0058,  0.0051,  0.0020, -0.0027, -0.0016, -0.0017, -0.0074])\n",
      "tensor([-2.8101e-04,  5.2164e-05,  3.1042e-03, -2.8726e-03,  2.8220e-03,\n",
      "         1.3934e-03, -2.3195e-03, -1.8987e-03])\n",
      "tensor([ 2.3243e-04, -1.7449e-03,  1.4051e-04,  4.7281e-03,  6.0928e-04,\n",
      "        -5.7006e-04, -3.3588e-03, -3.6565e-05])\n",
      "tensor([-0.0041,  0.0030,  0.0020, -0.0017, -0.0063,  0.0025,  0.0044,  0.0002])\n",
      "tensor([-0.0006,  0.0041,  0.0073,  0.0040, -0.0035,  0.0008, -0.0060, -0.0061])\n",
      "tensor([-0.0008,  0.0070,  0.0040,  0.0030, -0.0004, -0.0054, -0.0062, -0.0011])\n",
      "tensor([ 0.0014,  0.0007, -0.0036, -0.0024,  0.0047, -0.0042,  0.0045, -0.0012])\n",
      "tensor([-0.0060,  0.0128,  0.0080,  0.0052, -0.0098, -0.0026, -0.0071, -0.0004])\n",
      "tensor([ 0.0039, -0.0056, -0.0058, -0.0012,  0.0015, -0.0020,  0.0051,  0.0040])\n",
      "tensor([-0.0009,  0.0003,  0.0008,  0.0014, -0.0007,  0.0008,  0.0030, -0.0047])\n",
      "tensor([ 0.0016, -0.0037, -0.0030, -0.0018,  0.0048,  0.0002,  0.0028, -0.0010])\n",
      "tensor([ 0.0010, -0.0007, -0.0027, -0.0011, -0.0030,  0.0012,  0.0004,  0.0049])\n",
      "tensor([-0.0018,  0.0030, -0.0012, -0.0022,  0.0031, -0.0018, -0.0028,  0.0037])\n",
      "tensor([ 0.0011,  0.0009, -0.0008,  0.0010, -0.0042,  0.0009, -0.0017,  0.0027])\n",
      "tensor([ 6.2091e-05,  1.7380e-04, -9.7335e-04,  7.3120e-04,  1.2253e-03,\n",
      "         3.9175e-03, -4.2379e-03, -8.9865e-04])\n",
      "tensor([ 0.0004, -0.0017, -0.0025, -0.0039, -0.0023,  0.0037,  0.0042,  0.0021])\n",
      "tensor([-0.0005,  0.0017, -0.0007, -0.0016,  0.0010,  0.0034, -0.0052,  0.0018])\n",
      "tensor([-0.0003,  0.0005, -0.0022, -0.0019, -0.0006,  0.0018,  0.0007,  0.0020])\n",
      "tensor([-2.1680e-04,  2.7628e-04,  3.2158e-04, -1.1827e-05,  4.5483e-04,\n",
      "        -1.9736e-03, -5.1754e-04,  1.6671e-03])\n",
      "tensor([ 0.0013,  0.0006, -0.0005, -0.0023, -0.0006,  0.0021, -0.0003, -0.0003])\n",
      "tensor([-0.0034, -0.0009,  0.0012,  0.0150, -0.0049,  0.0037, -0.0064, -0.0043])\n",
      "tensor([ 1.9417e-03, -5.3915e-03, -7.4512e-03, -5.7611e-03,  7.1188e-03,\n",
      "         2.4120e-03,  7.1547e-03, -2.3270e-05])\n",
      "tensor([ 0.0009,  0.0026,  0.0011, -0.0044,  0.0044,  0.0058,  0.0008, -0.0112])\n",
      "tensor([-0.0007, -0.0030, -0.0044,  0.0068,  0.0051, -0.0062,  0.0080, -0.0055])\n",
      "tensor([-7.0245e-06,  1.3598e-03,  1.1810e-03,  1.0550e-03, -5.5825e-04,\n",
      "         5.7681e-03, -8.1027e-04, -7.9885e-03])\n",
      "tensor([-0.0029,  0.0045,  0.0030, -0.0027, -0.0006,  0.0022,  0.0007, -0.0043])\n",
      "tensor([ 0.0018,  0.0030,  0.0020,  0.0037, -0.0058, -0.0044, -0.0025,  0.0021])\n",
      "tensor([-0.0002,  0.0018,  0.0010,  0.0015,  0.0019, -0.0007, -0.0043, -0.0011])\n",
      "tensor([-0.0003,  0.0027,  0.0029,  0.0014, -0.0081, -0.0003,  0.0024, -0.0007])\n",
      "tensor([ 1.7083e-03, -1.4715e-03,  1.4829e-03,  2.7943e-03,  2.1548e-03,\n",
      "         6.6151e-05, -3.7274e-03, -3.0075e-03])\n",
      "tensor([-6.1186e-05,  1.2519e-03, -6.6974e-04, -7.2676e-04,  4.9036e-04,\n",
      "        -9.6542e-04, -1.6697e-03,  2.3505e-03])\n",
      "tensor([-3.8235e-04,  1.0945e-03, -1.0590e-03, -1.5436e-04, -4.9315e-05,\n",
      "         1.4138e-03, -3.8654e-03,  3.0021e-03])\n",
      "tensor([ 0.0011,  0.0002, -0.0014, -0.0006,  0.0002,  0.0013,  0.0017, -0.0025])\n",
      "tensor([-6.3921e-06,  1.7696e-03,  3.5711e-04,  1.4598e-03,  4.4646e-03,\n",
      "        -2.9921e-03, -1.5028e-03, -3.5499e-03])\n",
      "tensor([ 0.0017,  0.0034,  0.0029,  0.0011, -0.0091, -0.0017,  0.0045, -0.0027])\n",
      "tensor([-0.0015,  0.0037,  0.0016,  0.0016, -0.0021, -0.0027, -0.0009,  0.0004])\n",
      "tensor([-1.1588e-03, -7.9365e-06,  2.5675e-03,  5.6749e-04, -3.3686e-03,\n",
      "         2.0730e-03,  5.5599e-04, -1.2285e-03])\n",
      "tensor([-4.9598e-05,  1.3304e-03,  6.2271e-04,  4.4982e-04, -4.3830e-04,\n",
      "         6.4270e-03, -3.3808e-03, -4.9612e-03])\n",
      "tensor([-0.0006,  0.0003,  0.0001, -0.0002, -0.0028, -0.0029,  0.0034,  0.0025])\n",
      "tensor([ 0.0012, -0.0006,  0.0002, -0.0002,  0.0003, -0.0014,  0.0016, -0.0011])\n",
      "tensor([ 0.0018, -0.0042, -0.0029, -0.0013,  0.0016,  0.0025,  0.0021,  0.0004])\n",
      "tensor([ 0.0007, -0.0055, -0.0051, -0.0020, -0.0031,  0.0048,  0.0044,  0.0060])\n",
      "tensor([ 0.0006, -0.0050, -0.0055, -0.0036,  0.0070,  0.0033,  0.0055, -0.0023])\n",
      "tensor([-0.0007,  0.0022,  0.0038,  0.0015, -0.0019, -0.0009, -0.0003, -0.0038])\n",
      "tensor([-0.0002, -0.0002, -0.0012, -0.0010,  0.0003,  0.0018,  0.0012, -0.0007])\n",
      "tensor([-0.0008, -0.0009, -0.0012, -0.0010,  0.0005,  0.0017, -0.0021,  0.0039])\n",
      "tensor([ 0.0005,  0.0020,  0.0016,  0.0007, -0.0045, -0.0011,  0.0002,  0.0007])\n",
      "tensor([-0.0009,  0.0004,  0.0005,  0.0006, -0.0036,  0.0053, -0.0002, -0.0020])\n",
      "tensor([ 0.0012, -0.0005,  0.0009,  0.0024,  0.0040,  0.0002, -0.0032, -0.0050])\n",
      "tensor([-0.0007,  0.0046,  0.0049,  0.0037, -0.0040,  0.0014, -0.0065, -0.0034])\n",
      "tensor([ 2.2049e-03, -1.2700e-03, -1.2719e-03,  3.0511e-05, -9.1894e-04,\n",
      "         2.4849e-03,  8.5856e-04, -2.1180e-03])\n",
      "tensor([-9.1088e-04, -1.9235e-03, -2.2363e-04,  8.8580e-05,  1.0787e-03,\n",
      "         1.7071e-03, -3.2092e-03,  3.3928e-03])\n",
      "tensor([ 0.0017, -0.0050, -0.0064, -0.0016,  0.0053,  0.0017,  0.0016,  0.0027])\n",
      "tensor([ 0.0009, -0.0028, -0.0024, -0.0017,  0.0044,  0.0011, -0.0011,  0.0015])\n",
      "tensor([ 0.0004, -0.0029, -0.0027, -0.0020,  0.0014,  0.0029,  0.0021,  0.0008])\n",
      "tensor([ 0.0020, -0.0059, -0.0062, -0.0020,  0.0041,  0.0034,  0.0036,  0.0009])\n",
      "tensor([ 0.0019, -0.0055, -0.0052, -0.0032,  0.0046,  0.0027,  0.0016,  0.0031])\n",
      "tensor([ 2.1851e-04, -8.9023e-04,  5.6932e-04,  5.5795e-04,  6.6910e-05,\n",
      "         6.2856e-05, -1.8627e-03,  1.2774e-03])\n",
      "tensor([ 6.3169e-04, -9.1236e-04, -4.1776e-04, -1.7608e-06,  1.5296e-03,\n",
      "        -2.7723e-03,  5.8624e-04,  1.3566e-03])\n",
      "tensor([ 0.0017, -0.0027, -0.0029, -0.0003,  0.0009,  0.0015, -0.0005,  0.0025])\n",
      "tensor([ 0.0009, -0.0023, -0.0010, -0.0007, -0.0029,  0.0023,  0.0013,  0.0024])\n",
      "tensor([ 0.0004, -0.0014, -0.0013, -0.0014,  0.0036, -0.0012,  0.0039, -0.0026])\n"
     ]
    }
   ],
   "source": [
    "for param in get_params(model_dp_, 'arch'):\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('automl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7613325ebbb1f9384bc508bcd9660c9ee63eceea61e5a048c9a384ab1815eea9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
