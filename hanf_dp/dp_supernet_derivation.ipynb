{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from opacus import PrivacyEngine\n",
    "import torch.nn.functional as F\n",
    "from operations import *\n",
    "from genotypes import PRIMITIVES\n",
    "from genotypes import Genotype\n",
    "from opacus.grad_sample import GradSampleModule, register_grad_sampler\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "from typing import Dict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding sample-wise Gradients for Supernet\n",
    "\n",
    "In this notebook we derive how we come up with sample-wise gradients for our supernet we use to perform differentiable NAS. For this we note that the supernet is based on a convex, weighted combination of operations which are all applied to the same input, that is each _mixed operation_ is defined as follows:\n",
    "\\begin{equation}\n",
    "    m = \\sum_{o \\in O} \\alpha_o \\cdot o(x)\n",
    "\\end{equation}\n",
    "Each $o$ is a convolution/pooling operation or a regular neural network, thus opacus already knows how to compute sample-based gradients for all parameters of each $o$. Thus, with a smart design of our supernet-architecture we can avoid the computation of sample-wise gradients for all the operations we have in use and pass the heavy lifting to opacus. The problem then reduces to providing opacus with sample-wise gradients w.r.t $\\alpha_o$ for each $o$.\n",
    "\n",
    "For this, let's see how we compute the gradients of an arbitrary loss w.r.t. the alpha-parameters in a simple setup: We only have 3 operations, each associated with a certaing weight $\\alpha_o$. The mixed operation is then followed by a linear transformation producing the output, thus the network reads:\n",
    "\\begin{equation}\n",
    "    \\hat{y} = \\bigg(\\sum_{o \\in O} \\alpha_o \\cdot o(x) \\bigg) \\cdot \\mathbf{W}\n",
    "\\end{equation}\n",
    "\n",
    "The following shwos the forward pass of such a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [torch.randn(1, 5) for _ in range(0, 4)]\n",
    "alphas = nn.Parameter(torch.ones(4) / 4, requires_grad=True)\n",
    "W = nn.Parameter(torch.randn(5, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxed_alphas = torch.softmax(alphas, dim=0)\n",
    "softmaxed_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6962]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mop = sum(alphas[i] * X[i] for i in range(0, 4))\n",
    "y = torch.matmul(mop, W)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed with computing a loss and updating the parameters. To keep things easy, let's pretend our loss is just the sum of all elements in our output. We then can compute the gradients w.r.t. each $\\alpha_o$ easily by calling the backward-method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6128, 0.7298, 0.3566, 0.0854])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.sum(y)\n",
    "l.backward()\n",
    "alphas.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient can be expressed as follows:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\ell}{\\partial \\alpha_o} = \\sum_{i=1}^n \\frac{\\partial \\ell}{\\partial z^{(0)}_k} \\cdot \\frac{\\partial z^{(0)}_k}{\\partial \\alpha_o}\n",
    "\\end{equation}\n",
    "Here we sum over all gradients of our $n$ outputs w.r.t. $\\alpha_o$. $z^{(0)}_k$ denotes the $k$-th element of the last layer (output). Expanding this further yields:\n",
    "\\begin{align}\n",
    "    \\frac{\\partial \\ell}{\\partial \\alpha_o} = & \\sum_{i=1}^n \\frac{\\partial \\ell}{\\partial z^{(0)}_k} \\cdot \\sum_{d=1}^{|L_1|} \\mathbf{W}^{(1)}_{d k} \\cdot \\frac{\\partial z_d^{(1)}}{\\partial \\alpha_o} \\\\\n",
    "    & \\sum_{i=1}^n \\frac{\\partial \\ell}{\\partial z^{(0)}_k} \\cdot \\sum_{d=1}^{|L_1|} \\mathbf{W}^{(1)}_{d k} \\cdot \\frac{\\partial \\sum_{o \\in O} \\big( \\alpha_o o(z^{(2)}) \\big)}{\\partial \\alpha_o} \\\\\n",
    "    &  \\sum_{i=1}^n \\frac{\\partial \\ell}{\\partial z^{(0)}_k} \\cdot \\sum_{d=1}^{|L_1|} \\mathbf{W}^{(1)}_{d k} \\cdot o(z^{(2)})_d\n",
    "\\end{align}\n",
    "Since $\\frac{\\partial \\ell}{\\partial z^{(0)}_k} = 1$ we obtain:\n",
    "\\begin{align}\n",
    "    \\frac{\\partial \\ell}{\\partial \\alpha_o} & = \\sum_{k=1}^n \\sum_{d=1}^{|L_1|} \\mathbf{W}_{d k}^{(1)} \\cdot o(z^{(2)})_d \\\\\n",
    "\\end{align}\n",
    "This reduces to:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\ell}{\\partial \\alpha_o} = \\sum_{k=1}^n \\big(\\mathbf{W}^{(1)^T}\\big)_k \\cdot o(z^{(2)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6128]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 * X[0].matmul(W) # 1 = derivative w.r.t. the output of mixed operation, rest as derived above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the Minimal Example\n",
    "Now we will start extending the above approach. As you can see computing the gradients and deriving the forumlas can get cumbersome really quickly. That's why we should use opacus' capabilities of computing sample-wise gradients for as many modules as possible. Below we define 3 operations that will be used withing our NAS-approach. For these modules opacus already knows how to compute sample-wise gradients, thus we can just go ahead and use them as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Op1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Op2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Op3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "PRIMS = [Op1, Op2, Op3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tricky part is the mixed operation-module. If we would use plain pytorch, we just could go ahead and build one `MixedOp` module taking care of calling each operation and compute the convex combination of the operation's output. However, since opacus does not know the MixedOp-module it cannot compute the sample-wise gradients w.r.t. the alpha-parameters. Thus we have to tell opacus how to compute the gradients. For this opacus provides the activations (input to our module) and the gradients w.r.t. the outputs of our module. If we would go for a \"plain-pytorch approach\" this would require us to compute the gradients w.r.t. alpha-parameters and the model-parameters of each operation \"by hand\". Since this is cumbersome and not error-prone, we split up the MixedOp into two parts: One `ParallelOp` which does nothing but applying each operation on the same input data and a `MixedOp` (please don't get confused by the naming) which just cares about computing the convex combination of the oerpation's output computed by the ParallelOp. \n",
    "\n",
    "This way we can compute the gradients w.r.t. the alphas easily by just applying the same reasoning as above. The gradients can then be computed by computing the vector-product of the activations and the gradients w.r.t. the MixedOp (which are both provided by opacus). Mathematically this can be expressed as follows assuming we have an $n \\times i$-dimensional real activation matrix $\\mathbf{a}$ and an $i$-dimensional real vector $\\nabla \\mathbf{m}$ representing the gradients w.r.t. our mixed operation. We have an activation of $n \\times i$ because we compute the convex combination of $n$ operations, each having producing outputs of dimension $i$. Since each of the $n$ operations is associated with a weight $\\alpha_j$, we aim to compute the gradient w.r.t. each of the $n$ weights, thus we aim to obtain a $n$-dimensional gradient vector for one sample and a $B \\times n$-dimensional gradient matrix for a batch of size $B$. We can easily compute this using a einsum:\n",
    "\\begin{equation}\n",
    "    \\nabla_{j} \\alpha_m = \\sum_{k=1}^i \\nabla \\mathbf{m}_m \\cdot \\mathbf{a}_{m k}\n",
    "\\end{equation}\n",
    "Here $\\nabla_j \\alpha_m$ is the $j$-th element of the gradient vector w.r.t. to the alphas associated with operation $m$. As we can see this is just a more general version of the equation we've derived above.\n",
    "\n",
    "But this is not all we have to do: Remember we have the ParallelOp computing all the operation's outputs. This module does not have any parameters, thus there is nothing we can update and so there are also no gradients for this module. Thus we can tell opacus that there is nothing to compute in the backward pass. Details on the implementation can be obtained below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelOp(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(ParallelOp, self).__init__()\n",
    "        self._ops = nn.ModuleList()\n",
    "        for primitive in PRIMS:\n",
    "            self._ops.append(primitive())\n",
    "\n",
    "    def forward(self, x):\n",
    "        operation_outs = []\n",
    "        for op in self._ops:\n",
    "            out = op(x)\n",
    "            operation_outs.append(out)\n",
    "        return torch.stack(operation_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedOp(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MixedOp, self).__init__()\n",
    "        self.alphas = nn.Parameter(torch.zeros(len(PRIMS)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = torch.softmax(self.alphas, 0)\n",
    "        return sum(w * op_out for w, op_out in zip(weights, x))\n",
    "\n",
    "    def arch_params(self):\n",
    "        return [self.alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLayer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/opacus/privacy_engine.py:130: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mixed_op = MixedOp()\n",
    "net = nn.Sequential(ParallelOp(), mixed_op, LastLayer())\n",
    "optim = torch.optim.SGD(mixed_op.arch_params(), 0.01)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0,), (1,))])\n",
    "train_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=True, transform=transform)\n",
    "val_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=False, transform=transform)\n",
    "train_loader = DataLoader(train_data, 64)\n",
    "val_loader = DataLoader(val_data, 64)\n",
    "\n",
    "@register_grad_sampler(ParallelOp)\n",
    "def grad_sampler_parallel_op(layer: MixedOp, activations: torch.Tensor, backprops: torch.Tensor):\n",
    "    return {}\n",
    "\n",
    "@register_grad_sampler(MixedOp)\n",
    "def grad_sampler_mixed_op(layer: MixedOp, activations: torch.Tensor, backprops: torch.Tensor):\n",
    "    grad = torch.einsum('nbi,bi->nb', activations, backprops)\n",
    "    ret = {\n",
    "        layer.alphas: grad\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "pe = PrivacyEngine()\n",
    "netc = deepcopy(net)\n",
    "net_, optim_, train_loader_ = pe.make_private(module=net, optimizer=optim, data_loader=train_loader, noise_multiplier=1., max_grad_norm=1.)\n",
    "x_first, y_first = next(iter(train_loader_))\n",
    "x_first = x_first.reshape((x_first.shape[0], 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_first = torch.randn((64, 784))\n",
    "y_first = torch.ones(64, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for pnetc, pnet_ in zip(netc.parameters(), net_.parameters()):\n",
    "    print(torch.all(pnet_.data == pnetc.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "y_pred = net_(x_first)\n",
    "l = loss(y_pred, y_first)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = netc(x_first)\n",
    "l = loss(y_pred, y_first)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for pnetc, pnet_ in zip(netc.parameters(), net_.parameters()):\n",
    "    print(torch.all(pnetc.grad.data == pnet_.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Does this make sense? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Conv2d(1, 3, 3) # 26x25x3\n",
    "        self.pool = nn.MaxPool2d(5) # 5x5x3\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc2 = nn.Linear(75, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Net"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "optim = torch.optim.SGD(net.parameters(), 0.01)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0,), (1,))])\n",
    "train_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=True, transform=transform)\n",
    "val_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=False, transform=transform)\n",
    "train_loader = DataLoader(train_data, 64)\n",
    "val_loader = DataLoader(val_data, 64)\n",
    "type(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/opacus/privacy_engine.py:130: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "opacus.grad_sample.grad_sample_module.GradSampleModule"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PrivacyEngine()\n",
    "net_, optim_, train_loader_ = pe.make_private(module=net, optimizer=optim, data_loader=train_loader, noise_multiplier=1., max_grad_norm=1.)\n",
    "type(net_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "for e in range(0, 10):\n",
    "    running_loss = 0\n",
    "    for x, y in train_loader_:\n",
    "        #x_ = x.reshape(x.shape[0], 784)\n",
    "        y_hat = net_(x)\n",
    "        l = loss(y_hat, y)\n",
    "        running_loss += l\n",
    "\n",
    "        optim_.zero_grad()\n",
    "        l.backward()\n",
    "        optim_.step()\n",
    "    \n",
    "    print(f\"Loss: {running_loss / len(train_loader_)} \\t Epoch: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedOp(nn.Module):\n",
    "\n",
    "  def __init__(self, C, stride):\n",
    "    super(MixedOp, self).__init__()\n",
    "    self._ops = nn.ModuleList()\n",
    "    self.alphas = nn.Parameter(torch.zeros(len(PRIMITIVES)), requires_grad=True)\n",
    "    for primitive in PRIMITIVES:\n",
    "      op = OPS[primitive](C, stride, False)\n",
    "      if 'pool' in primitive:\n",
    "        op = nn.Sequential(op, nn.GroupNorm(num_groups=1, num_channels=C, affine=False))\n",
    "      self._ops.append(op)\n",
    "\n",
    "  def forward(self, x):\n",
    "    weights = torch.softmax(self.alphas, 0)\n",
    "    return sum(w * op(x) for w, op in zip(weights, self._ops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.mop = MixedOp(1, 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mop(x)\n",
    "        print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "optim = torch.optim.SGD(net.parameters(), 0.01)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0,), (1,))])\n",
    "train_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=True, transform=transform)\n",
    "val_data = torchvision.datasets.FashionMNIST('../../datasets/femnist/', download=True, train=False, transform=transform)\n",
    "train_loader = DataLoader(train_data, 64)\n",
    "val_loader = DataLoader(val_data, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_grad_sampler(MixedOp)\n",
    "def grad_sampler(layer: MixedOp, activations: torch.Tensor, backprops: torch.Tensor):\n",
    "    print(activations.shape)\n",
    "    print(backprops.shape)\n",
    "    return torch.einsum('n..i,n..j->nij')\n",
    "\n",
    "pe = PrivacyEngine()\n",
    "net_, optim_, train_loader_ = pe.make_private(module=net, optimizer=optim, data_loader=train_loader, noise_multiplier=1., max_grad_norm=1.)\n",
    "x_first, y_first = next(iter(train_loader_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 1, 28, 28])\n",
      "torch.Size([69, 1, 28, 28])\n",
      "torch.Size([69, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "einsum(): must specify the equation string and at least one operand, or at least one operand and its subscripts list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb Cell 17\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_pred \u001b[39m=\u001b[39m net_(x_first)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m l \u001b[39m=\u001b[39m loss(y_pred, y_first)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m l\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/opacus/grad_sample/grad_sample_module.py:352\u001b[0m, in \u001b[0;36mGradSampleModule.capture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    345\u001b[0m activations, backprops \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrearrange_grad_samples(\n\u001b[1;32m    346\u001b[0m     module\u001b[39m=\u001b[39mmodule,\n\u001b[1;32m    347\u001b[0m     backprops\u001b[39m=\u001b[39mbackprops,\n\u001b[1;32m    348\u001b[0m     loss_reduction\u001b[39m=\u001b[39mloss_reduction,\n\u001b[1;32m    349\u001b[0m     batch_first\u001b[39m=\u001b[39mbatch_first,\n\u001b[1;32m    350\u001b[0m )\n\u001b[1;32m    351\u001b[0m grad_sampler_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGRAD_SAMPLERS[\u001b[39mtype\u001b[39m(module)]\n\u001b[0;32m--> 352\u001b[0m grad_samples \u001b[39m=\u001b[39m grad_sampler_fn(module, activations, backprops)\n\u001b[1;32m    353\u001b[0m \u001b[39mfor\u001b[39;00m param, gs \u001b[39min\u001b[39;00m grad_samples\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    354\u001b[0m     create_or_accumulate_grad_sample(\n\u001b[1;32m    355\u001b[0m         param\u001b[39m=\u001b[39mparam, grad_sample\u001b[39m=\u001b[39mgs, max_batch_len\u001b[39m=\u001b[39mmodule\u001b[39m.\u001b[39mmax_batch_len\n\u001b[1;32m    356\u001b[0m     )\n",
      "\u001b[1;32m/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb Cell 17\u001b[0m in \u001b[0;36mgrad_sampler\u001b[0;34m(layer, activations, backprops)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(activations\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(backprops\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m'\u001b[39;49m\u001b[39mn..i,n..j->nij\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/torch/functional.py:284\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39m# This wrapper exists to support variadic args.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meinsum(): must specify the equation string and at least one operand, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    285\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mor at least one operand and its subscripts list\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    287\u001b[0m equation \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    288\u001b[0m operands \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: einsum(): must specify the equation string and at least one operand, or at least one operand and its subscripts list"
     ]
    }
   ],
   "source": [
    "y_pred = net_(x_first)\n",
    "l = loss(y_pred, y_first)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([-0.0086, -0.0018,  0.0022,  0.0026, -0.0081,  0.0201,  0.0095, -0.0159])\n",
      "Loss: 0.0025060009211301804 \t Epoch: 0\n",
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([-0.0086, -0.0018,  0.0022,  0.0026, -0.0081,  0.0201,  0.0095, -0.0159])\n",
      "Loss: 0.0025060009211301804 \t Epoch: 1\n",
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([-0.0086, -0.0018,  0.0022,  0.0026, -0.0081,  0.0201,  0.0095, -0.0159])\n",
      "Loss: 0.0025060009211301804 \t Epoch: 2\n",
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([-0.0086, -0.0018,  0.0022,  0.0026, -0.0081,  0.0201,  0.0095, -0.0159])\n",
      "Loss: 0.0025060009211301804 \t Epoch: 3\n",
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([-0.0086, -0.0018,  0.0022,  0.0026, -0.0081,  0.0201,  0.0095, -0.0159])\n",
      "Loss: 0.0025060009211301804 \t Epoch: 4\n",
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([-0.0086, -0.0018,  0.0022,  0.0026, -0.0081,  0.0201,  0.0095, -0.0159])\n",
      "Loss: 0.0025060009211301804 \t Epoch: 5\n",
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([-0.0086, -0.0018,  0.0022,  0.0026, -0.0081,  0.0201,  0.0095, -0.0159])\n",
      "Loss: 0.0025060009211301804 \t Epoch: 6\n",
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([-0.0086, -0.0018,  0.0022,  0.0026, -0.0081,  0.0201,  0.0095, -0.0159])\n",
      "Loss: 0.0025060009211301804 \t Epoch: 7\n",
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([-0.0086, -0.0018,  0.0022,  0.0026, -0.0081,  0.0201,  0.0095, -0.0159])\n",
      "Loss: 0.0025060009211301804 \t Epoch: 8\n",
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([-0.0086, -0.0018,  0.0022,  0.0026, -0.0081,  0.0201,  0.0095, -0.0159])\n",
      "Loss: 0.0025060009211301804 \t Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "for e in range(0, 10):\n",
    "    running_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        #x_ = x.reshape(x.shape[0], 784)\n",
    "        y_hat = net(x)\n",
    "        l = loss(y_hat, y)\n",
    "        running_loss += l\n",
    "\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        print(net.mop.alphas.grad)\n",
    "        break\n",
    "        optim.step()\n",
    "    \n",
    "    print(f\"Loss: {running_loss / len(train_loader)} \\t Epoch: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/opacus/privacy_engine.py:130: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "opacus.grad_sample.grad_sample_module.GradSampleModule"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PrivacyEngine()\n",
    "net_, optim_, train_loader_ = pe.make_private(module=net, optimizer=optim, data_loader=train_loader, noise_multiplier=1., max_grad_norm=1.)\n",
    "type(net_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (53) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb Cell 12\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m l\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(net\u001b[39m.\u001b[39mmop\u001b[39m.\u001b[39malphas\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/opacus/grad_sample/grad_sample_module.py:352\u001b[0m, in \u001b[0;36mGradSampleModule.capture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    345\u001b[0m activations, backprops \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrearrange_grad_samples(\n\u001b[1;32m    346\u001b[0m     module\u001b[39m=\u001b[39mmodule,\n\u001b[1;32m    347\u001b[0m     backprops\u001b[39m=\u001b[39mbackprops,\n\u001b[1;32m    348\u001b[0m     loss_reduction\u001b[39m=\u001b[39mloss_reduction,\n\u001b[1;32m    349\u001b[0m     batch_first\u001b[39m=\u001b[39mbatch_first,\n\u001b[1;32m    350\u001b[0m )\n\u001b[1;32m    351\u001b[0m grad_sampler_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGRAD_SAMPLERS[\u001b[39mtype\u001b[39m(module)]\n\u001b[0;32m--> 352\u001b[0m grad_samples \u001b[39m=\u001b[39m grad_sampler_fn(module, activations, backprops)\n\u001b[1;32m    353\u001b[0m \u001b[39mfor\u001b[39;00m param, gs \u001b[39min\u001b[39;00m grad_samples\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    354\u001b[0m     create_or_accumulate_grad_sample(\n\u001b[1;32m    355\u001b[0m         param\u001b[39m=\u001b[39mparam, grad_sample\u001b[39m=\u001b[39mgs, max_batch_len\u001b[39m=\u001b[39mmodule\u001b[39m.\u001b[39mmax_batch_len\n\u001b[1;32m    356\u001b[0m     )\n",
      "\u001b[1;32m/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb Cell 12\u001b[0m in \u001b[0;36mgrad_sampler\u001b[0;34m(layer, activations, backprops)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m@register_grad_sampler\u001b[39m(MixedOp)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrad_sampler\u001b[39m(layer: MixedOp, activations: torch\u001b[39m.\u001b[39mTensor, backprops: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(activations \u001b[39m-\u001b[39;49m x_first)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(backprops)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (53) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "for e in range(0, 10):\n",
    "    running_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        #x_ = x.reshape(x.shape[0], 784)\n",
    "        y_hat = net(x)\n",
    "        l = loss(y_hat, y)\n",
    "        running_loss += l\n",
    "\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        print(net.mop.alphas.grad.shape)\n",
    "        break\n",
    "        optim.step()\n",
    "    \n",
    "    print(f\"Loss: {running_loss / len(train_loader)} \\t Epoch: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedOp(nn.Module):\n",
    "\n",
    "  def __init__(self, C, stride):\n",
    "    super(MixedOp, self).__init__()\n",
    "    self._ops = nn.ModuleList()\n",
    "    for primitive in PRIMITIVES:\n",
    "      op = OPS[primitive](C, stride, False)\n",
    "      if 'pool' in primitive:\n",
    "        op = nn.Sequential(op, nn.GroupNorm(num_groups=1, num_channels=C, affine=False))\n",
    "      self._ops.append(op)\n",
    "\n",
    "  def forward(self, x, weights):\n",
    "    return sum(w * op(x) for w, op in zip(weights, self._ops))\n",
    "\n",
    "\n",
    "class Cell(nn.Module):\n",
    "\n",
    "  def __init__(self, steps, multiplier, C_prev_prev, C_prev, C, reduction, reduction_prev):\n",
    "    super(Cell, self).__init__()\n",
    "    self.reduction = reduction\n",
    "\n",
    "    if reduction_prev:\n",
    "      self.preprocess0 = FactorizedReduce(C_prev_prev, C, affine=False)\n",
    "    else:\n",
    "      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0, affine=False)\n",
    "    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0, affine=False)\n",
    "    self._steps = steps\n",
    "    self._multiplier = multiplier\n",
    "\n",
    "    self._ops = nn.ModuleList()\n",
    "    self._bns = nn.ModuleList()\n",
    "    for i in range(self._steps):\n",
    "      for j in range(2+i):\n",
    "        stride = 2 if reduction and j < 2 else 1\n",
    "        op = MixedOp(C, stride)\n",
    "        self._ops.append(op)\n",
    "\n",
    "  def forward(self, s0, s1, weights):\n",
    "    s0 = self.preprocess0(s0)\n",
    "    s1 = self.preprocess1(s1)\n",
    "\n",
    "    states = [s0, s1]\n",
    "    offset = 0\n",
    "    for i in range(self._steps):\n",
    "      s = sum(self._ops[offset+j](h, weights[offset+j]) for j, h in enumerate(states))\n",
    "      offset += len(states)\n",
    "      states.append(s)\n",
    "\n",
    "    return torch.cat(states[-self._multiplier:], dim=1)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "  def __init__(self, C, num_classes, layers, criterion, device, in_channels=3, steps=4, multiplier=4, stem_multiplier=3):\n",
    "    super(Network, self).__init__()\n",
    "    self._C = C\n",
    "    self._num_classes = num_classes\n",
    "    self._layers = layers\n",
    "    self._criterion = criterion\n",
    "    self._steps = steps\n",
    "    self._multiplier = multiplier\n",
    "    self.device = device\n",
    "\n",
    "    C_curr = stem_multiplier*C\n",
    "    self.stem = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, C_curr, 3, padding=1, bias=False),\n",
    "      nn.GroupNorm(num_groups=1, num_channels=C_curr),\n",
    "    )\n",
    " \n",
    "    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C\n",
    "    self.cells = nn.ModuleList()\n",
    "    reduction_prev = False\n",
    "    for i in range(layers):\n",
    "      if i in [layers//3, 2*layers//3]:\n",
    "        C_curr *= 2\n",
    "        reduction = True\n",
    "      else:\n",
    "        reduction = False\n",
    "      cell = Cell(steps, multiplier, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\n",
    "      reduction_prev = reduction\n",
    "      self.cells += [cell]\n",
    "      C_prev_prev, C_prev = C_prev, multiplier*C_curr\n",
    "\n",
    "    self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "    self.classifier = nn.Linear(C_prev, num_classes)\n",
    "\n",
    "    self._initialize_alphas()\n",
    "\n",
    "  def new(self):\n",
    "    model_new = Network(self._C, self._num_classes, self._layers, self._criterion, self.device).to(self.device)\n",
    "    for x, y in zip(model_new.arch_parameters(), self.arch_parameters()):\n",
    "        x.data.copy_(y.data)\n",
    "    return model_new\n",
    "\n",
    "  def forward(self, input):\n",
    "    s0 = s1 = self.stem(input)\n",
    "    for i, cell in enumerate(self.cells):\n",
    "      if cell.reduction:\n",
    "        weights = F.softmax(self.alphas_reduce, dim=-1)\n",
    "      else:\n",
    "        weights = F.softmax(self.alphas_normal, dim=-1)\n",
    "      s0, s1 = s1, cell(s0, s1, weights)\n",
    "    out = self.global_pooling(s1)\n",
    "    logits = self.classifier(out.view(out.size(0),-1))\n",
    "    return logits\n",
    "\n",
    "  def _initialize_alphas(self):\n",
    "    k = sum(1 for i in range(self._steps) for n in range(2+i))\n",
    "    num_ops = len(PRIMITIVES)\n",
    "\n",
    "    self.alphas_normal = nn.Parameter(1e-3*torch.randn(k, num_ops).to(self.device), requires_grad=True)\n",
    "    self.alphas_reduce = nn.Parameter(1e-3*torch.randn(k, num_ops).to(self.device), requires_grad=True)\n",
    "    self._arch_parameters = [\n",
    "      self.alphas_normal,\n",
    "      self.alphas_reduce,\n",
    "    ]\n",
    "\n",
    "  def arch_parameters(self):\n",
    "    return self._arch_parameters\n",
    "\n",
    "  def genotype(self):\n",
    "\n",
    "    def _parse(weights):\n",
    "      gene = []\n",
    "      n = 2\n",
    "      start = 0\n",
    "      for i in range(self._steps):\n",
    "        end = start + n\n",
    "        W = weights[start:end].copy()\n",
    "        edges = sorted(range(i + 2), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[:2]\n",
    "        for j in edges:\n",
    "          k_best = None\n",
    "          for k in range(len(W[j])):\n",
    "            if k != PRIMITIVES.index('none'):\n",
    "              if k_best is None or W[j][k] > W[j][k_best]:\n",
    "                k_best = k\n",
    "          gene.append((PRIMITIVES[k_best], j))\n",
    "        start = end\n",
    "        n += 1\n",
    "      return gene\n",
    "\n",
    "    gene_normal = _parse(F.softmax(self.alphas_normal, dim=-1).data.cpu().numpy())\n",
    "    gene_reduce = _parse(F.softmax(self.alphas_reduce, dim=-1).data.cpu().numpy())\n",
    "\n",
    "    concat = range(2+self._steps-self._multiplier, self._steps+2)\n",
    "    genotype = Genotype(\n",
    "      normal=gene_normal, normal_concat=concat,\n",
    "      reduce=gene_reduce, reduce_concat=concat\n",
    "    )\n",
    "    return genotype\n",
    "\n",
    "@register_grad_sampler(Network)\n",
    "def compute_linear_grad_sample(\n",
    "    layer: Network, activations: torch.Tensor, backprops: torch.Tensor\n",
    ") -> Dict[nn.Parameter, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes per sample gradients for ``nn.Linear`` layer\n",
    "    Args:\n",
    "        layer: Layer\n",
    "        activations: Activations\n",
    "        backprops: Backpropagations\n",
    "    \"\"\"\n",
    "    print(backprops.shape)\n",
    "    print(activations.shape)\n",
    "    # TODO: We receive dL/dN where N is our network and input into the network, i.e. we would have to compute each gradient manually.\n",
    "    #   How can we circumvent this? Probably we have to break down the cell-structure and implement the cells directly in the network(?)\n",
    "    # TODO: Try to register Cell grad_sampler and see if it works out. \n",
    "    gs = torch.einsum(\"ni,n...kj->nkj\", backprops, activations)\n",
    "    ret = {layer.classifier.weight: gs}\n",
    "    if layer.classifier.bias is not None:\n",
    "        ret[layer.classifier.bias] = torch.einsum(\"n...k->nk\", backprops)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cpu')\n",
    "model = Network(16, 10, 7, criterion, device, in_channels=1) # Cell(4, 3, 16, 36, 48, False, False)\n",
    "optim = torch.optim.SGD(model.parameters(), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/opacus/privacy_engine.py:130: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "opacus.grad_sample.grad_sample_module.GradSampleModule"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pengine = PrivacyEngine()\n",
    "model_, optim_, train_loader_ = pengine.make_private(module=model, optimizer=optim, data_loader=train_loader, noise_multiplier=1., max_grad_norm=1.)\n",
    "type(model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (28) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb Cell 9\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     l\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/dev/automl/HANF/hanf_dp/test.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mrunning_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Epoch: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/opacus/grad_sample/grad_sample_module.py:354\u001b[0m, in \u001b[0;36mGradSampleModule.capture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    352\u001b[0m grad_samples \u001b[39m=\u001b[39m grad_sampler_fn(module, activations, backprops)\n\u001b[1;32m    353\u001b[0m \u001b[39mfor\u001b[39;00m param, gs \u001b[39min\u001b[39;00m grad_samples\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 354\u001b[0m     create_or_accumulate_grad_sample(\n\u001b[1;32m    355\u001b[0m         param\u001b[39m=\u001b[39;49mparam, grad_sample\u001b[39m=\u001b[39;49mgs, max_batch_len\u001b[39m=\u001b[39;49mmodule\u001b[39m.\u001b[39;49mmax_batch_len\n\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    358\u001b[0m \u001b[39m# Detect end of current batch processing and switch accumulation\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m# mode from sum to stacking. Used for RNNs and tied parameters\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m# (See #417 for details)\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/opacus/grad_sample/grad_sample_module.py:49\u001b[0m, in \u001b[0;36mcreate_or_accumulate_grad_sample\u001b[0;34m(param, grad_sample, max_batch_len)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mCreates a ``_current_grad_sample`` attribute in the given parameter, or adds to it\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39mif the ``_current_grad_sample`` attribute already exists.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m    layer: nn.Module parameter belongs to\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(param, \u001b[39m\"\u001b[39m\u001b[39m_current_grad_sample\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m     param\u001b[39m.\u001b[39m_current_grad_sample[: grad_sample\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m grad_sample\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     param\u001b[39m.\u001b[39m_current_grad_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\n\u001b[1;32m     52\u001b[0m         torch\u001b[39m.\u001b[39mSize([max_batch_len]) \u001b[39m+\u001b[39m grad_sample\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:],\n\u001b[1;32m     53\u001b[0m         device\u001b[39m=\u001b[39mgrad_sample\u001b[39m.\u001b[39mdevice,\n\u001b[1;32m     54\u001b[0m         dtype\u001b[39m=\u001b[39mgrad_sample\u001b[39m.\u001b[39mdtype,\n\u001b[1;32m     55\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (28) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "for e in range(0, 10):\n",
    "    running_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        y_hat = model(x)\n",
    "        l = loss(y_hat, y)\n",
    "        running_loss += l\n",
    "\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    print(f\"Loss: {running_loss / len(train_loader)} \\t Epoch: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_module.alphas_normal\n",
      "0\n",
      "_module.alphas_reduce\n",
      "0\n",
      "_module.stem.0.weight\n",
      "0\n",
      "_module.stem.1.weight\n",
      "0\n",
      "_module.stem.1.bias\n",
      "0\n",
      "_module.cells.0.preprocess0.op.1.weight\n",
      "0\n",
      "_module.cells.0.preprocess1.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.0._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.1._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.2._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.3._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.4._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.5._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.6._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.7._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.8._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.9._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.10._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.11._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.12._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.0._ops.13._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1.preprocess0.op.1.weight\n",
      "0\n",
      "_module.cells.1.preprocess1.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.0._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.1._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.2._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.3._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.4._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.5._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.6._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.7._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.8._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.9._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.10._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.11._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.12._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.1._ops.13._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2.preprocess0.op.1.weight\n",
      "0\n",
      "_module.cells.2.preprocess1.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.0._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.1._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.2._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.3._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.4._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.5._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.6._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.7._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.8._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.9._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.10._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.11._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.12._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.2._ops.13._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3.preprocess0.conv_1.weight\n",
      "0\n",
      "_module.cells.3.preprocess0.conv_2.weight\n",
      "0\n",
      "_module.cells.3.preprocess1.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.0._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.1._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.2._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.3._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.4._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.5._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.6._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.7._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.8._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.9._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.10._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.11._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.12._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.3._ops.13._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4.preprocess0.op.1.weight\n",
      "0\n",
      "_module.cells.4.preprocess1.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.0._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.1._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.2._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.3._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.4._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.5._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.6._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.7._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.8._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.9._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.3.conv_1.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.3.conv_2.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.10._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.11._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.12._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.4._ops.13._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5.preprocess0.conv_1.weight\n",
      "0\n",
      "_module.cells.5.preprocess0.conv_2.weight\n",
      "0\n",
      "_module.cells.5.preprocess1.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.0._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.1._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.2._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.3._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.4._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.5._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.6._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.7._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.8._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.9._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.10._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.11._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.12._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.5._ops.13._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6.preprocess0.op.1.weight\n",
      "0\n",
      "_module.cells.6.preprocess1.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.0._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.1._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.2._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.3._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.4._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.5._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.6._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.7._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.8._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.9._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.10._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.11._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.12._ops.7.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.4.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.4.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.4.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.4.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.5.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.5.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.5.op.5.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.5.op.6.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.6.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.6.op.2.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.7.op.1.weight\n",
      "0\n",
      "_module.cells.6._ops.13._ops.7.op.2.weight\n",
      "0\n",
      "_module.classifier.weight\n",
      "0\n",
      "_module.classifier.bias\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for n, p in model_.named_parameters():\n",
    "    print(n)\n",
    "    print(p._forward_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('automl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7613325ebbb1f9384bc508bcd9660c9ee63eceea61e5a048c9a384ab1815eea9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
